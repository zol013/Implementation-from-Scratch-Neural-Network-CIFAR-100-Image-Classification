{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os, gzip\n",
    "import yaml\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradient\n",
    "from constants import *\n",
    "from train import *\n",
    "from gradient import *\n",
    "import util\n",
    "import argparse\n",
    "import neuralnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(path):\n",
    "    \"\"\"\n",
    "    Loads the config yaml from the specified path\n",
    "    args:\n",
    "        path - Complete path of the config yaml file to be loaded\n",
    "    returns:\n",
    "        yaml - yaml object containing the config file\n",
    "    \"\"\"\n",
    "    return yaml.load(open(path, 'r'), Loader=yaml.SafeLoader)\n",
    "\n",
    "\n",
    "\n",
    "def normalize_data(inp):\n",
    "    \"\"\"\n",
    "    Normalizes inputs (on per channel basis of every image) here to have 0 mean and unit variance.\n",
    "    This will require reshaping to seprate the channels and then undoing it while returning\n",
    "    args:\n",
    "        inp : N X d 2D array where N is the number of examples and d is the number of dimensions\n",
    "    returns:\n",
    "        normalized inp: N X d 2D array\n",
    "    \"\"\"\n",
    "    l = len(inp)\n",
    "    inp = inp.reshape(l,3,-1)\n",
    "    u = np.mean(inp, axis=2, keepdims=True)\n",
    "    sd = np.std(inp, axis=2, keepdims=True)\n",
    "    inp_normalized = (inp - u) / sd\n",
    "    return inp_normalized.reshape(l,-1)\n",
    "\n",
    "\n",
    "\n",
    "def one_hot_encoding(labels, num_classes=20):\n",
    "    \"\"\"\n",
    "    Encodes labels using one hot encoding.\n",
    "    args:\n",
    "        labels : N dimensional 1D array where N is the number of examples\n",
    "        num_classes: Number of distinct labels that we have (20/100 for CIFAR-100)\n",
    "    returns:\n",
    "        oneHot : N X num_classes 2D array\n",
    "    \"\"\"\n",
    "    return np.eye(num_classes)[labels].squeeze()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_minibatches(dataset, batch_size=64):\n",
    "    \"\"\"\n",
    "        Generates minibatches of the dataset\n",
    "        args:\n",
    "            dataset : 2D Array N (examples) X d (dimensions)\n",
    "            batch_size: mini batch size. Default value=64\n",
    "        yields:\n",
    "            (X,y) tuple of size=batch_size\n",
    "        \"\"\"\n",
    "\n",
    "    X, y = dataset\n",
    "    l_idx, r_idx = 0, batch_size\n",
    "    while r_idx < len(X):\n",
    "        yield X[l_idx:r_idx], y[l_idx:r_idx]\n",
    "        l_idx, r_idx = r_idx, r_idx + batch_size\n",
    "\n",
    "    yield X[l_idx:], y[l_idx:]\n",
    "\n",
    "\n",
    "def calculateCorrect(y,t):  #Feel free to use this function to return accuracy instead of number of correct predictions\n",
    "    \"\"\"\n",
    "    Calculates the number of correct predictions\n",
    "    args:\n",
    "        y: Predicted Probabilities\n",
    "        t: Labels in one hot encoding\n",
    "    returns:\n",
    "        the number of correct predictions\n",
    "    \"\"\"\n",
    "    return np.sum(np.argmax(y, axis=1) == np.argmax(t,axis = 1)) / len(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def append_bias(X):\n",
    "    \"\"\"\n",
    "    Appends bias to the input\n",
    "    args:\n",
    "        X (N X d 2D Array)\n",
    "    returns:\n",
    "        X_bias (N X (d+1)) 2D Array\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    return np.c_[np.ones(N), X]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plots(trainEpochLoss, trainEpochAccuracy, valEpochLoss, valEpochAccuracy, earlyStop):\n",
    "\n",
    "    \"\"\"\n",
    "    Helper function for creating the plots\n",
    "    earlyStop is the epoch at which early stop occurred and will correspond to the best model. e.g. earlyStop=-1 means the last epoch was the best one\n",
    "    \"\"\"\n",
    "\n",
    "    fig1, ax1 = plt.subplots(figsize=((24, 12)))\n",
    "    epochs = np.arange(1,len(trainEpochLoss)+1,1)\n",
    "    ax1.plot(epochs, trainEpochLoss, 'r', label=\"Training Loss\")\n",
    "    ax1.plot(epochs, valEpochLoss, 'g', label=\"Validation Loss\")\n",
    "    plt.scatter(epochs[earlyStop],valEpochLoss[earlyStop],marker='x', c='g',s=400,label='Early Stop Epoch')\n",
    "    plt.xticks(ticks=np.arange(min(epochs),max(epochs)+1,10), fontsize=35 )\n",
    "    plt.yticks(fontsize=35)\n",
    "    ax1.set_title('Loss Plots', fontsize=35.0)\n",
    "    ax1.set_xlabel('Epochs', fontsize=35.0)\n",
    "    ax1.set_ylabel('Cross Entropy Loss', fontsize=35.0)\n",
    "    ax1.legend(loc=\"upper right\", fontsize=35.0)\n",
    "    plt.savefig(constants.saveLocation+\"loss.eps\")\n",
    "    plt.show()\n",
    "\n",
    "    fig2, ax2 = plt.subplots(figsize=((24, 12)))\n",
    "    ax2.plot(epochs, trainEpochAccuracy, 'r', label=\"Training Accuracy\")\n",
    "    ax2.plot(epochs, valEpochAccuracy, 'g', label=\"Validation Accuracy\")\n",
    "    plt.scatter(epochs[earlyStop], valEpochAccuracy[earlyStop], marker='x', c='g', s=400, label='Early Stop Epoch')\n",
    "    plt.xticks(ticks=np.arange(min(epochs),max(epochs)+1,10), fontsize=35)\n",
    "    plt.yticks(fontsize=35)\n",
    "    ax2.set_title('Accuracy Plots', fontsize=35.0)\n",
    "    ax2.set_xlabel('Epochs', fontsize=35.0)\n",
    "    ax2.set_ylabel('Accuracy', fontsize=35.0)\n",
    "    ax2.legend(loc=\"lower right\", fontsize=35.0)\n",
    "    plt.savefig(constants.saveLocation+\"accuarcy.eps\")\n",
    "    plt.show()\n",
    "\n",
    "    #Saving the losses and accuracies for further offline use\n",
    "    pd.DataFrame(trainEpochLoss).to_csv(constants.saveLocation+\"trainEpochLoss.csv\")\n",
    "    pd.DataFrame(valEpochLoss).to_csv(constants.saveLocation+\"valEpochLoss.csv\")\n",
    "    pd.DataFrame(trainEpochAccuracy).to_csv(constants.saveLocation+\"trainEpochAccuracy.csv\")\n",
    "    pd.DataFrame(valEpochAccuracy).to_csv(constants.saveLocation+\"valEpochAccuracy.csv\")\n",
    "\n",
    "\n",
    "\n",
    "def createTrainValSplit(x_train,y_train):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates the train-validation split (80-20 split for train-val). Please shuffle the data before creating the train-val split.\n",
    "    \"\"\"\n",
    "    length = len(y_train)\n",
    "    order = np.random.permutation(length)\n",
    "\n",
    "    fold_width = length // 5\n",
    "    x_validation = x_train[order[:fold_width]]\n",
    "    y_validation = y_train[order[:fold_width]]\n",
    "    xtrain = x_train[order[fold_width:]]\n",
    "    ytrain = y_train[order[fold_width:]]\n",
    "    return xtrain, ytrain, x_validation, y_validation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Loads, splits our dataset- CIFAR-100 into train, val and test sets and normalizes them\n",
    "    args:\n",
    "        path: Path to cifar-100 dataset\n",
    "    returns:\n",
    "        train_normalized_images, train_one_hot_labels, val_normalized_images, val_one_hot_labels,  test_normalized_images, test_one_hot_labels\n",
    "    \"\"\"\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "\n",
    "    cifar_path = os.path.join(path, constants.cifar100_directory)\n",
    "\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "\n",
    "    images_dict = unpickle(os.path.join(cifar_path, \"train\"))\n",
    "    data = images_dict[b'data']\n",
    "    label = images_dict[b'coarse_labels']\n",
    "    train_labels.extend(label)\n",
    "    train_images.extend(data)\n",
    "    train_images = np.array(train_images)\n",
    "    train_labels = np.array(train_labels).reshape((len(train_labels),-1))\n",
    "    train_images, train_labels, val_images, val_labels = createTrainValSplit(train_images,train_labels)\n",
    "\n",
    "\n",
    "    train_normalized_images =  normalize_data(train_images)\n",
    "    train_one_hot_labels = one_hot_encoding(train_labels, 20)\n",
    "\n",
    "    val_normalized_images = normalize_data(val_images)\n",
    "    val_one_hot_labels = one_hot_encoding(val_labels, 20)\n",
    "\n",
    "    test_images_dict = unpickle(os.path.join(cifar_path, \"test\"))\n",
    "    test_data = test_images_dict[b'data']\n",
    "    test_labels = test_images_dict[b'coarse_labels']\n",
    "    test_images = np.array(test_data)\n",
    "    test_labels = np.array(test_labels).reshape((len(test_labels), -1))\n",
    "    test_normalized_images = normalize_data(test_images)\n",
    "    test_one_hot_labels = one_hot_encoding(test_labels, 20)\n",
    "    return train_normalized_images, train_one_hot_labels, val_normalized_images, val_one_hot_labels,  test_normalized_images, test_one_hot_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "\n",
    "    # Read the required config\n",
    "    # Create different config files for different experiments\n",
    "    configFile = None #Will contain the name of the config file to be loaded\n",
    "    if (args.experiment == 'test_gradients'):  #3b\n",
    "        configFile = 'config_test_gradient.yaml' # Create a config file for 3b and change None to the config file name\n",
    "    elif(args.experiment=='test_momentum'):  #3c\n",
    "        configFile = \"config_3c.yaml\" # Create a config file for 3c and change None to the config file name\n",
    "    elif (args.experiment == 'test_regularization'): #3d\n",
    "        configFile = None # Create a config file for 3d and change None to the config file name\n",
    "    elif (args.experiment == 'test_activation'): #3e\n",
    "        configFile = None # Create a config file for 3e and change None to the config file name\n",
    "    elif (args.experiment == 'test_hidden_units'):  #3f-i\n",
    "        configFile = None # Create a config file for 3f-i and change None to the config file name\n",
    "    elif (args.experiment == 'test_hidden_layers'):  #3f-ii\n",
    "        configFile = None # Create a config file for 3f-ii and change None to the config file name\n",
    "    elif (args.experiment == 'test_100_classes'):  #3g\n",
    "        configFile = None # Create a config file for 3g and change None to the config file name. Please make the necessaty changes to load_data()\n",
    "        # in util.py first before running this experiment\n",
    "\n",
    "    # Load the data\n",
    "    x_train, y_train, x_valid, y_valid, x_test, y_test = util.load_data(path='')  # Set datasetDir in constants.py\n",
    "\n",
    "    # Load the configuration from the corresponding yaml file. Specify the file path and name\n",
    "    config = util.load_config('configs/' + configFile) # Set configYamlPath, configFile  in constants.py\n",
    "\n",
    "    if(args.experiment == 'test_gradients'):\n",
    "        gradient.checkGradient(x_train,y_train,config)\n",
    "        return 1\n",
    "\n",
    "    # Create a Neural Network object which will be our model\n",
    "    model = neuralnet.Neuralnetwork(config)\n",
    "\n",
    "    # train the model. Use train.py's train method for this\n",
    "    model = modeltrain(model, x_train, y_train, x_valid, y_valid, config)\n",
    "\n",
    "    # test the model. Use train.py's modelTest method for this\n",
    "    test_acc, test_loss =  modelTest(model, x_test, y_test)\n",
    "\n",
    "    # Print test accuracy and test loss\n",
    "    print('Test Accuracy:', test_acc, ' Test Loss:', test_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1943\n",
      "0.2124\n",
      "0.2289\n",
      "0.2361\n",
      "0.2423\n",
      "0.2486\n",
      "0.2512\n",
      "0.2603\n",
      "0.2594\n",
      "0.258\n",
      "0.2601\n",
      "0.2577\n",
      "0.2586\n",
      "0.2521\n",
      "0.2521\n",
      "0.2544\n",
      "0.2493\n",
      "0.2474\n",
      "0.2503\n",
      "0.2463\n",
      "0.2431\n",
      "0.2445\n",
      "0.2439\n",
      "0.243\n",
      "0.2455\n",
      "Test Accuracy: 28622.009497161456  Test Loss: 0.2358\n"
     ]
    }
   ],
   "source": [
    "# Parse the input arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid, x_test, y_test = util.load_data('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
