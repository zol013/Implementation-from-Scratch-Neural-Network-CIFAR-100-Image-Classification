{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation():\n",
    "    \"\"\"\n",
    "    The class implements different types of activation functions for\n",
    "    your neural network layers.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, activation_type = \"sigmoid\"):\n",
    "        \"\"\"\n",
    "        TODO in case you want to add variables here\n",
    "        Initialize activation type and placeholders here.\n",
    "        \"\"\"\n",
    "        if activation_type not in [\"sigmoid\", \"tanh\", \"ReLU\",\"output\"]:   #output can be used for the final layer. Feel free to use/remove it\n",
    "            raise NotImplementedError(f\"{activation_type} is not implemented.\")\n",
    "\n",
    "        # Type of non-linear activation.\n",
    "        self.activation_type = activation_type\n",
    "\n",
    "        # Placeholder for input. This can be used for computing gradients.\n",
    "        self.x = None\n",
    "\n",
    "    def __call__(self, z):\n",
    "        \"\"\"\n",
    "        This method allows your instances to be callable.\n",
    "        \"\"\"\n",
    "        return self.forward(z)\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Compute the forward pass.\n",
    "        \"\"\"\n",
    "        if self.activation_type == \"sigmoid\":\n",
    "            return self.sigmoid(z)\n",
    "\n",
    "        elif self.activation_type == \"tanh\":\n",
    "            return self.tanh(z)\n",
    "\n",
    "        elif self.activation_type == \"ReLU\":\n",
    "            return self.ReLU(z)\n",
    "\n",
    "        elif self.activation_type == \"output\":\n",
    "            return self.output(z)\n",
    "\n",
    "    def backward(self, z):\n",
    "        \"\"\"\n",
    "        Compute the backward pass.\n",
    "        \"\"\"\n",
    "        if self.activation_type == \"sigmoid\":\n",
    "            return self.grad_sigmoid(z)\n",
    "\n",
    "        elif self.activation_type == \"tanh\":\n",
    "            return self.grad_tanh(z)\n",
    "\n",
    "        elif self.activation_type == \"ReLU\":\n",
    "            return self.grad_ReLU(z)\n",
    "\n",
    "        elif self.activation_type == \"output\":\n",
    "            return self.grad_output(z)\n",
    "\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Implement the sigmoid activation here.\n",
    "        \"\"\"\n",
    "        return 1/(1 + np.exp(-a))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Implement tanh here.\n",
    "        \"\"\"\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Implement ReLU here.\n",
    "        \"\"\"\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def output(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Implement softmax function here.\n",
    "        Remember to take care of the overflow condition.\n",
    "        \"\"\"\n",
    "        x = np.exp(a)\n",
    "        return x/np.sum(x, axis = 1, keepdims = True)\n",
    "\n",
    "    def grad_sigmoid(self,x):\n",
    "        \"\"\"\n",
    "        TODO: Compute the gradient for sigmoid here.\n",
    "        \"\"\"\n",
    "        val = self.sigmoid(x)\n",
    "        return val*(1 - val)\n",
    "\n",
    "    def grad_tanh(self,x):\n",
    "        \"\"\"\n",
    "        TODO: Compute the gradient for tanh here.\n",
    "        \"\"\"\n",
    "        val = self.tanh(x)\n",
    "        return 1 - val**2\n",
    "\n",
    "    def grad_ReLU(self,x):\n",
    "        \"\"\"\n",
    "        TODO: Compute the gradient for ReLU here.\n",
    "        \"\"\"\n",
    "        x[x<= 0] = 0\n",
    "        x[x> 0] = 1\n",
    "        return x\n",
    " \n",
    "    def grad_output(self, x):\n",
    "        \"\"\"\n",
    "        Deliberately returning 1 for output layer case since we don't multiply by any activation for final layer's delta. Feel free to use/disregard it\n",
    "        \"\"\"\n",
    "\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"\n",
    "    This class implements Fully Connected layers for your neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_units, out_units, activation, weightType):\n",
    "        \"\"\"\n",
    "        TODO in case you want to add variables here\n",
    "        Define the architecture and create placeholders.\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "\n",
    "        self.w = None\n",
    "        if (weightType == 'random'):\n",
    "            self.w = 0.01 * np.random.random((in_units + 1, out_units))\n",
    "\n",
    "        self.x = None    # Save the input to forward in this\n",
    "        self.a = None    #output without activation\n",
    "        self.z = None    # Output After Activation\n",
    "        self.activation = activation   #Activation function\n",
    "        self.v = np.zeros((in_units + 1, out_units))\n",
    "\n",
    "\n",
    "        self.dw = 0  # Save the gradient w.r.t w in this. You can have bias in w itself or uncomment the next line and handle it separately\n",
    "        # self.d_b = None  # Save the gradient w.r.t b in this\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Make layer callable.\n",
    "        \"\"\"\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Compute the forward pass (activation of the weighted input) through the layer here and return it.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.a = x @ self.w\n",
    "        self.z = self.activation(self.a)\n",
    "        self.dw = self.activation.backward(self.a)\n",
    "        return self.z\n",
    "\n",
    "    def backward(self, deltaCur, learning_rate, momentum_gamma, regularization, gradReqd=True):\n",
    "        \"\"\"\n",
    "        TODO: Write the code for backward pass. This takes in gradient from its next layer as input and\n",
    "        computes gradient for its weights and the delta to pass to its previous layers. gradReqd is used to specify whether to update the weights i.e. whether self.w should\n",
    "        be updated after calculating self.dw\n",
    "        The delta expression (that you prove in PA2 part1) for any layer consists of delta and weights from the next layer and derivative of the activation function\n",
    "        of weighted inputs i.e. g'(a) of that layer. Hence deltaCur (the input parameter) will have to be multiplied with the derivative of the activation function of the weighted\n",
    "        input of the current layer to actually get the delta for the current layer. Remember, this is just one way of interpreting it and you are free to interpret it any other way.\n",
    "        Feel free to change the function signature if you think of an alternative way to implement the delta calculation or the backward pass.\n",
    "        gradReqd=True means update self.w with self.dw. gradReqd=False can be helpful for Q-3b\n",
    "        \"\"\"\n",
    "        \n",
    "        delta = self.dw * deltaCur\n",
    "\n",
    "        if gradReqd:\n",
    "            gradient = -learning_rate * regularization * self.w\n",
    "            for i in range(len(delta)):\n",
    "                gradient = gradient + learning_rate * np.tensordot(self.x[i] , delta[i], axes = 0)\n",
    "            self.v = momentum_gamma * self.v + gradient/len(delta)\n",
    "            self.w = self.w + self.v\n",
    "\n",
    "        return delta @ self.w.T[:,1:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuralnetwork():\n",
    "    def __init__(self, config):\n",
    "        self.layers = []  # Store all layers in this list.\n",
    "        self.num_layers = len(config['layer_specs']) - 1  # Set num layers here\n",
    "        self.x = None  # Save the input to forward in this\n",
    "        self.y = None        # For saving the output vector of the model\n",
    "        self.targets = None  # For saving the targets\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.momentum_gamma = config['momentum_gamma']\n",
    "        self.regularization = config['L2_penalty']\n",
    "\n",
    "        # Add layers specified by layer_specs.\n",
    "        for i in range(self.num_layers):\n",
    "            if i < self.num_layers - 1:\n",
    "                self.layers.append(\n",
    "                    Layer(config['layer_specs'][i], config['layer_specs'][i + 1], Activation(config['activation']),\n",
    "                          config[\"weight_type\"]))\n",
    "            elif i == self.num_layers - 1:\n",
    "                self.layers.append(Layer(config['layer_specs'][i], config['layer_specs'][i + 1], Activation(\"output\"),\n",
    "                                         config[\"weight_type\"]))\n",
    "\n",
    "        \n",
    "    def __call__(self, x, targets=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Make NeuralNetwork callable.\n",
    "        \"\"\"\n",
    "        return self.forward(x, targets)\n",
    "\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        self.x = x\n",
    "        inputs = x\n",
    "        for i in range(self.num_layers):\n",
    "            inputs = self.layers[i](util.append_bias(inputs))\n",
    "        self.y = inputs\n",
    "\n",
    "\n",
    "        self.targets = targets\n",
    "        loss = self.loss(self.y, targets)\n",
    "        return loss, util.calculateCorrect(self.y, targets)\n",
    "\n",
    "\n",
    "    def loss(self, logits, targets):\n",
    "        '''\n",
    "        TODO: compute the categorical cross-entropy loss and return it.\n",
    "        '''\n",
    "        \n",
    "        entropy = np.multiply(np.log(logits), targets)\n",
    "        return - entropy\n",
    "        \n",
    "        \n",
    "\n",
    "    def backward(self, gradReqd=True):\n",
    "        delta = self.targets - self.y\n",
    "        for i in range(self.num_layers-1, -1, -1):\n",
    "            delta = self.layers[i].backward(delta, self.learning_rate, self.momentum_gamma, self.regularization,gradReqd)\n",
    "\n",
    "    \n",
    "                \n",
    "\n",
    ", \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
